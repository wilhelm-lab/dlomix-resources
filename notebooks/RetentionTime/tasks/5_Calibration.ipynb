{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Time Preidiction \n",
    "\n",
    "This notebook is prepared to be run in Google [Colaboratory](https://colab.research.google.com/). In order to train the model faster, please change the runtime of Colab to use Hardware Accelerator, either GPU or TPU.\n",
    "\n",
    "This is an extension of the original walkthrough example available [here](https://github.com/wilhelm-lab/dlomix-resources/blob/main/notebooks/RetentionTime/Example_RTModel_Walkthrough_colab.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Data Split\n",
    "Similar to the initial notebook, we will initialize our model and train it. The target here is to experiment with different data splits and observe the impact on the performance and whether it reflects a realistic evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the mlomix package in the current environment using pip\n",
    "\n",
    "!python -m pip install -q dlomix==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dlomix\n",
    "from dlomix.models import RetentionTimePredictor\n",
    "import tensorflow as tf\n",
    "from dlomix.eval import TimeDeltaMetric\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter project name for weights and biases\n",
    "project_name = 'dlomix_retention_time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a dataset, creates the model, and trains it. You should try with the two available data splits (`feature_a` and `feature_b`). Please Refer to the initial notebook to analyze the results.\n",
    "\n",
    "Hint: Use the paths available below. Description for features is as follows:\n",
    "- suffix `_A`: feature A\n",
    "- suffix `_B`: feature B\n",
    "\n",
    "**Different features span through different ranges the absolute losses/metrics might be misleading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATAPATH_A = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/main/example_datasets/RetentionTime/feature_a/proteomeTools_train_val_a.csv'\n",
    "TRAIN_DATAPATH_B = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/main/example_datasets/RetentionTime/feature_b/proteomeTools_train_val_b.csv'\n",
    "\n",
    "TEST_DATAPATH_A = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/main/example_datasets/RetentionTime/feature_a/proteomeTools_test_a.csv'\n",
    "TEST_DATAPATH_B = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/main/example_datasets/RetentionTime/feature_b/proteomeTools_test_b.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data import RetentionTimeDataset\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "rtdata = RetentionTimeDataset(data_source=TRAIN_DATAPATH_B,target_col='retention_time',\n",
    "                              seq_length=30, batch_size=BATCH_SIZE, val_ratio=0.2)\n",
    "\n",
    "\n",
    "# this is the test dataset object, do not forget to change it to the respective suffix (A or B)\n",
    "# when you change the training dataset\n",
    "\n",
    "test_rtdata = RetentionTimeDataset(data_source=TEST_DATAPATH_B,target_col='retention_time',\n",
    "                              seq_length=30, batch_size=BATCH_SIZE, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter weights and biases run name. Make sure that different splits rates have different run names.\n",
    "wandb.init(project=project_name, name='data_split_b')\n",
    "\n",
    "# create model\n",
    "model = RetentionTimePredictor(seq_length=30)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# compile the model  with the optimizer and the metrics we want to use, we can add our custom time-delta metric\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='mse', metrics=['mean_absolute_error', TimeDeltaMetric()])\n",
    "\n",
    "history = model.fit(rtdata.train_data, validation_data=rtdata.val_data, epochs=15,\n",
    "                    callbacks=[WandbCallback(save_model=False)])\n",
    "\n",
    "\n",
    "# Mark the run as finished\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "After analyzing the results, can you figure out what is wrong with these splits and how different are they from each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
