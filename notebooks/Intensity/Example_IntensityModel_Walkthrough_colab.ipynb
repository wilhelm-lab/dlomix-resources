{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YWkUVjVr7qJ"
   },
   "source": [
    "# Intensity Preidiction \n",
    "\n",
    "This notebook is prepared to be run in Google [Colaboratory](https://colab.research.google.com/). In order to train the model faster, please change the runtime of Colab to use Hardware Accelerator, either GPU or TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3DlTOq3r7qM"
   },
   "source": [
    "This notebook presents a short walkthrough the process of reading a dataset and training a model for retention time prediction. The dataset is an example dataset extracted from a ProteomTools dataset generated in the **Chair of Bioanalytics** at the **Technical University of Munich**.\n",
    "\n",
    "The framework being used is a custom wrapper on top of Keras/TensorFlow. The working name of the package is for now DLOmix -  `dlomix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import functools\n",
    "import dlomix.losses as losses\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def reshape_dims(array):\n",
    "    n, dims = array.shape\n",
    "    assert dims == 174\n",
    "    nlosses = 1\n",
    "    return array.reshape(\n",
    "        [array.shape[0], 30 - 1, 2, nlosses, 3]\n",
    "    )\n",
    "\n",
    "\n",
    "def reshape_flat(array):\n",
    "    s = array.shape\n",
    "    flat_dim = [s[0], functools.reduce(lambda x, y: x * y, s[1:], 1)]\n",
    "    return array.reshape(flat_dim)\n",
    "\n",
    "\n",
    "def normalize_base_peak(array):\n",
    "    # flat\n",
    "    maxima = array.max(axis=1)\n",
    "    array = array / maxima[:, numpy.newaxis]\n",
    "    return array\n",
    "\n",
    "\n",
    "def mask_outofrange(array, lengths, mask=-1.):\n",
    "    # dim\n",
    "    for i in range(array.shape[0]):\n",
    "        array[i, lengths[i] - 1 :, :, :, :] = mask\n",
    "    return array\n",
    "\n",
    "\n",
    "def mask_outofcharge(array, charges, mask=-1.):\n",
    "    # dim\n",
    "    for i in range(array.shape[0]):\n",
    "        if charges[i] < 3:\n",
    "            array[i, :, :, :, charges[i] :] = mask\n",
    "    return array\n",
    "\n",
    "\n",
    "def get_spectral_angle(true, pred, batch_size=600):\n",
    "\n",
    "    n = true.shape[0]\n",
    "    sa = numpy.zeros([n])\n",
    "\n",
    "    def iterate():\n",
    "        if n > batch_size:\n",
    "            for i in range(n // batch_size):\n",
    "                true_sample = true[i * batch_size : (i + 1) * batch_size]\n",
    "                pred_sample = pred[i * batch_size : (i + 1) * batch_size]\n",
    "                yield i, true_sample, pred_sample\n",
    "            i = n // batch_size\n",
    "            yield i, true[(i) * batch_size :], pred[(i) * batch_size :]\n",
    "        else:\n",
    "            yield 0, true, pred\n",
    "\n",
    "    for i, t_b, p_b in iterate():\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        with tf.compat.v1.Session() as s:\n",
    "            sa_graph = losses.masked_spectral_distance(t_b, p_b)\n",
    "            sa_b = 1 - s.run(sa_graph)\n",
    "            sa[i * batch_size : i * batch_size + sa_b.shape[0]] = sa_b\n",
    "    sa = numpy.nan_to_num(sa)\n",
    "    return sa\n",
    "\n",
    "\n",
    "def normalize_predictions(data, batch_size=600):\n",
    "    assert \"sequences\" in data\n",
    "    assert \"intensities_pred\" in data\n",
    "    assert \"precursor_charge_onehot\" in data\n",
    "\n",
    "    sequence_lengths = data[\"sequences\"].apply(lambda x: len(x))\n",
    "    intensities =  np.stack(data[\"intensities_pred\"].to_numpy()).astype(np.float32)\n",
    "    precursor_charge_onehot = np.stack(data[\"precursor_charge_onehot\"].to_numpy())\n",
    "    charges = list(precursor_charge_onehot.argmax(axis=1) + 1)\n",
    "\n",
    "    intensities[intensities < 0] = 0\n",
    "    intensities = reshape_dims(intensities)\n",
    "    intensities = mask_outofrange(intensities, sequence_lengths)\n",
    "    intensities = mask_outofcharge(intensities, charges)\n",
    "    intensities = reshape_flat(intensities)\n",
    "    m_idx = intensities == -1\n",
    "    intensities = normalize_base_peak(intensities)\n",
    "    intensities[m_idx] = -1\n",
    "    data[\"intensities_pred\"] = intensities\n",
    "\n",
    "    if \"intensities_raw\" in data:\n",
    "        data[\"spectral_angle\"] = get_spectral_angle(\n",
    "            np.stack(data[\"intensities_raw\"].to_numpy()).astype(np.float32), intensities, batch_size=batch_size\n",
    "        )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aO-69zbKsGey",
    "outputId": "2d7f5f84-aa06-4ec7-9a2b-c1f546031baf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install the DLOmix package in the current environment using pip\n",
    "\n",
    "!python -m pip install -q dlomix==0.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo7H9qzWr7qN"
   },
   "source": [
    "The available modules in the framework are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0CS0tFur7qN",
    "outputId": "16c0f94a-97ab-4445-ef6b-4ac28029e64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['META_DATA', 'constants', 'data', 'eval', 'layers', 'models', 'pipelines', 'reports', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dlomix\n",
    "from dlomix import constants, data, eval, layers, models, pipelines, reports, utils\n",
    "print([x for x in dir(dlomix) if not x.startswith(\"_\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsgPZb_Mr7qP"
   },
   "source": [
    "- `constants`: constants to be used in the framework (e.g. Aminoacid alphabet mapping)\n",
    "- `data`:  classes for representing dataset, wrappers around TensorFlow Dataset\n",
    "- `eval`: custom evaluation metrics implemented in Keras/TF to work as `metrics` for model training\n",
    "- `layers`: custom layer implementation required for the different models\n",
    "- `models`: different model implementations for Retention Time Prediction\n",
    "- `pipelines`: complete pipelines to run a task (e.g. Retention Time prediction)\n",
    "- `utils`: helper modules\n",
    "\n",
    "**Note**: reports and pipelines are work-in-progress, some funtionalities are not complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41qXroyKr7qP"
   },
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We can import the dataset class and create an object of type `RetentionTimeDataset`. This object wraps around TensorFlow dataset objects for training+validation or for testing. This can be specified by the arguments `val_ratio` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RiXz_epEr7qQ"
   },
   "outputs": [],
   "source": [
    "from dlomix.data import IntensityDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lzNXJ-s6r7qQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0        [0, 0, 1, 0, 0, 0]\n",
      "1        [0, 1, 0, 0, 0, 0]\n",
      "2        [0, 0, 1, 0, 0, 0]\n",
      "3        [0, 0, 1, 0, 0, 0]\n",
      "4        [0, 0, 1, 0, 0, 0]\n",
      "                ...        \n",
      "18259    [0, 1, 0, 0, 0, 0]\n",
      "18260    [0, 1, 0, 0, 0, 0]\n",
      "18261    [0, 0, 1, 0, 0, 0]\n",
      "18262    [0, 1, 0, 0, 0, 0]\n",
      "18263    [0, 0, 1, 0, 0, 0]\n",
      "Name: precursor_charge_onehot, Length: 18264, dtype: object\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATAPATH = 'D:/Compmass/proteomeTools_train_val.csv'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "int_data = IntensityDataset(data_source=TRAIN_DATAPATH,\n",
    "                              seq_length=30, batch_size=BATCH_SIZE,collision_energy_col='collision_energy', val_ratio=0.2, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UUzvcHGr7qR"
   },
   "source": [
    "Now we have an RT dataset that can be used directly with standard or custom `Keras` models. This wrapper contains the splits we chose when creating it. In our case, they are training and validation splits. To get the TF Dataset, we call the attributes `.train_data` and `.val_data`. The length is now in batches (i.e. `total examples = batch_size x len`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1l6YedCr7qS",
    "outputId": "16a1d57b-7720-4cb1-cdea-a32d9d2c3804"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Training examples', 14656)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"Training examples\", BATCH_SIZE * len(int_data.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEnVWjtRr7qT",
    "outputId": "95550088-76c5-4f1b-8181-dddb942d94b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Validation examples', 3712)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Validation examples\", BATCH_SIZE * len(int_data.val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWeVi0iar7qT"
   },
   "source": [
    "## 2. Model\n",
    "\n",
    "We can now create the model. We will use a simple Prediction with a conv1D encoder. It has the default working arguments, but most of the parameters can be customized.\n",
    "\n",
    "**Note**: Important is to ensure that the padding length used for the dataset object is equal to the sequence length passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q8SGTvfRr7qT"
   },
   "outputs": [],
   "source": [
    "from dlomix.models import PrositIntensityPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZqrsF6APr7qU"
   },
   "outputs": [],
   "source": [
    "model = PrositIntensityPredictor(seq_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adD60VwQr7qU"
   },
   "source": [
    "## 3. Training\n",
    "\n",
    "We can then train the model like a standard Keras model. The training parameters here are from Prosit, but other optimizer parameters can be used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IkPIHuWEr7qU"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from dlomix.eval import TimeDeltaMetric\n",
    "from dlomix.losses import masked_spectral_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xLy32wk7r7qU"
   },
   "outputs": [],
   "source": [
    "# create the optimizer object\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# compile the model  with the optimizer and the metrics we want to use, we can add our custom timedelta metric\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=masked_spectral_distance,\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtEUn_vdr7qV"
   },
   "source": [
    "We store the result of training so that we can explore the metrics and the losses later. We specify the number of epochs for training and pass the training and validation data as previously described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E14EcoYTr7qV",
    "outputId": "4455efd2-161b-4dff-e747-6db1724defbb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "229/229 [==============================] - ETA: 0s - loss: 0.6117 - mse: 1.9724WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [<tf.Tensor 'prosit_intensity_predictor/Cast:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'prosit_intensity_predictor/Cast_1:0' shape=(None, 6) dtype=float32>]\n",
      "Consider rewriting this model with the Functional API.\n",
      "encoded sequence:  Tensor(\"prosit_intensity_predictor/string_lookup/Identity:0\", shape=(None, 30), dtype=int64)\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'list'> input: [<tf.Tensor 'prosit_intensity_predictor/encoder_att/Sum_1:0' shape=(None, 512) dtype=float32>, <tf.Tensor 'prosit_intensity_predictor/sequential/meta_dense_do/Identity:0' shape=(None, 512) dtype=float32>]\n",
      "Consider rewriting this model with the Functional API.\n",
      "229/229 [==============================] - 119s 518ms/step - loss: 0.6117 - mse: 1.9724 - val_loss: 0.5544 - val_mse: 2.5445\n",
      "Epoch 2/15\n",
      " 11/229 [>.............................] - ETA: 1:45 - loss: 0.5634 - mse: 1.4458"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-48796ab7c315>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(int_data.train_data,\n\u001b[0;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                     epochs=15)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(int_data.train_data,\n",
    "                    validation_data=int_data.val_data,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oukZ4AyMr7qV"
   },
   "source": [
    "## 3. Testing and Reporting\n",
    "\n",
    "We can create a test dataset to test our model. Additionally, we can use the reporting module to produce plots and evaluate the model.\n",
    "\n",
    "Note: the reporting module is still in progress and some functionalities might easily break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ngz4zlnwr7qV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0        [0, 0, 1, 0, 0, 0]\n",
      "1        [0, 1, 0, 0, 0, 0]\n",
      "2        [0, 0, 1, 0, 0, 0]\n",
      "3        [0, 0, 1, 0, 0, 0]\n",
      "4        [0, 0, 1, 0, 0, 0]\n",
      "                ...        \n",
      "18259    [0, 1, 0, 0, 0, 0]\n",
      "18260    [0, 1, 0, 0, 0, 0]\n",
      "18261    [0, 0, 1, 0, 0, 0]\n",
      "18262    [0, 1, 0, 0, 0, 0]\n",
      "18263    [0, 0, 1, 0, 0, 0]\n",
      "Name: precursor_charge_onehot, Length: 18264, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# create the dataset object for test data\n",
    "\n",
    "TEST_DATAPATH = 'D:/Compmass/proteomeTools_test.csv'\n",
    "\n",
    "test_int_data = IntensityDataset(data_source=TEST_DATAPATH,\n",
    "                              seq_length=30, collision_energy_col='collision_energy',batch_size=32, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RrvR8Cl3r7qV"
   },
   "outputs": [],
   "source": [
    "# use model.predict from keras directly on the testdata\n",
    "\n",
    "predictions = model.predict(test_int_data.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00151483, -0.00016489, -0.00027107, ..., -0.02478559,\n",
       "        -0.02298378, -0.01914065],\n",
       "       [ 0.00018149, -0.00043896, -0.0032556 , ..., -0.03871916,\n",
       "        -0.03681742, -0.03799452],\n",
       "       [ 0.00161823, -0.00018393, -0.00028721, ..., -0.02620803,\n",
       "        -0.02486783, -0.02045185],\n",
       "       ...,\n",
       "       [ 0.00161323, -0.00018112, -0.00029086, ..., -0.02497605,\n",
       "        -0.0238777 , -0.01930104],\n",
       "       [ 0.0002696 , -0.00042114, -0.00283213, ..., -0.03069953,\n",
       "        -0.03000446, -0.02962921],\n",
       "       [ 0.00234061, -0.0003035 , -0.0004592 , ..., -0.03357218,\n",
       "        -0.03152551, -0.02862653]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['sequences'] = test_rtdata.sequences\n",
    "predictions_df['intensities_pred'] = predictions.tolist()\n",
    "predictions_df['precursor_charge_onehot'] = test_rtdata.precursor_charge.tolist()\n",
    "predictions_df['intensities_raw'] = test_rtdata.intensities.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_acc = normalize_predictions(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    18264.000000\n",
       "mean         0.487196\n",
       "std          0.214041\n",
       "min          0.049291\n",
       "25%          0.327691\n",
       "50%          0.449062\n",
       "75%          0.598523\n",
       "max          0.956674\n",
       "Name: spectral_angle, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_acc['spectral_angle'].describe()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Example_RTModel_Walkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
