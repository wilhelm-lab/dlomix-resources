{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fragment ion intensities Prediction \n",
    "\n",
    "This notebook is prepared to be run in Google [Colaboratory](https://colab.research.google.com/). In order to train the model faster, please change the runtime of Colab to use Hardware Accelerator, either GPU or TPU.\n",
    "\n",
    "This is an extension of the original walkthrough example available [here](https://github.com/wilhelm-lab/dlomix-resources/tree/tasks/intensity/notebooks/Intensity/Example_IntensityModel_Walkthrough_colab.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Data Split\n",
    "Similar to the initial notebook, we will initialize our model and train it. The target here is to experiment with different data splits and observe the impact on the performance and whether it reflects a realistic evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# install the mlomix package in the current environment using pip\n",
    "\n",
    "!python -m pip install -q git+https://github.com/wilhelm-lab/dlomix.git@feature/intensity_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dlomix\n",
    "from dlomix.models import PrositIntensityPredictor\n",
    "import tensorflow as tf\n",
    "from dlomix.losses import masked_spectral_distance, masked_pearson_correlation_distance\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a dataset, creates the model, and trains it. You should try with the two available data splits (`split_a` and `split_b`). Please Refer to the initial notebook to analyze the results.\n",
    "\n",
    "Hint: Use the paths available below. Description for splits is as follows:\n",
    "- suffix `_A`: split A\n",
    "- suffix `_B`: split B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATAPATH_A = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/tasks/intensity/example_datasets/Intensity/split_a/proteomeTools_train_val_a.csv'\n",
    "TRAIN_DATAPATH_B = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/tasks/intensity/example_datasets/Intensity/split_b/proteomeTools_train_val_b.csv'\n",
    "\n",
    "TEST_DATAPATH_A = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/tasks/intensity/example_datasets/Intensity/split_a/proteomeTools_test_a.csv'\n",
    "TEST_DATAPATH_B = 'https://raw.githubusercontent.com/wilhelm-lab/dlomix-resources/tasks/intensity/example_datasets/Intensity/split_b/proteomeTools_test_b.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.data import IntensityDataset\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "int_data = RetentionTimeDataset(data_source=TRAIN_DATAPATH_A,\n",
    "                              seq_length=30, collision_energy_col='collision_energy',batch_size=BATCH_SIZE, val_ratio=0.2)\n",
    "\n",
    "\n",
    "# this is the test dataset object, do not forget to change it to the respective suffix (A or B)\n",
    "# when you change the training dataset\n",
    "\n",
    "int_data = RetentionTimeDataset(data_source=TEST_DATAPATH_A,\n",
    "                              seq_length=30, collision_energy_col='collision_energy',batch_size=BATCH_SIZE, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter weights and biases run name. Make sure that different datasets splits have different names.\n",
    "wandb.init(project=project_name, name='splits_a')\n",
    "\n",
    "# create model\n",
    "model = PrositIntensityPredictor(seq_length=30)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# compile the model  with the optimizer and the metrics we want to use, we can add our custom time-delta metric\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss=masked_spectral_distance, metrics=[masked_pearson_correlation_distance,'mean_absolute_error', 'mse'])\n",
    "\n",
    "history = model.fit(int_data.train_data, validation_data=int_data.val_data, epochs=15,\n",
    "                    callbacks=[WandbCallback(save_model=False)])\n",
    "\n",
    "\n",
    "# Mark the run as finished\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "After analyzing the results, can you figure out what is wrong with these splits and how different are they from each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
